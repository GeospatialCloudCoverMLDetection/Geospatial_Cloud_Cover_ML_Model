{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-20T05:05:31.731916Z",
     "iopub.status.busy": "2025-04-20T05:05:31.731152Z",
     "iopub.status.idle": "2025-04-20T05:05:36.729744Z",
     "shell.execute_reply": "2025-04-20T05:05:36.729009Z",
     "shell.execute_reply.started": "2025-04-20T05:05:31.731887Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T05:36:07.554521Z",
     "iopub.status.busy": "2025-04-20T05:36:07.553636Z",
     "iopub.status.idle": "2025-04-20T08:32:21.058629Z",
     "shell.execute_reply": "2025-04-20T08:32:21.057465Z",
     "shell.execute_reply.started": "2025-04-20T05:36:07.554493Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import albumentations as A\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Custom Dataset for Cloud Detection\n",
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, metadata, root_dir, transform=None):\n",
    "        self.metadata = metadata\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.feature_dir = os.path.join(root_dir, 'train_features')\n",
    "        self.label_dir = os.path.join(root_dir, 'train_labels')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chip_id = self.metadata.iloc[idx]['chip_id']\n",
    "        feature_path = os.path.join(self.feature_dir, chip_id)\n",
    "        label_path = os.path.join(self.label_dir, f\"{chip_id}.tif\")\n",
    "\n",
    "        bands = ['B02', 'B03', 'B04', 'B08']  # Blue, Green, Red, NIR\n",
    "        features = []\n",
    "\n",
    "        for band in bands:\n",
    "            img_path = os.path.join(feature_path, f'{band}.tif')\n",
    "            with rasterio.open(img_path) as src:\n",
    "                feature = src.read(1).astype(np.float32)\n",
    "            features.append(feature)\n",
    "\n",
    "        image = np.stack(features, axis=-1)\n",
    "\n",
    "        with rasterio.open(label_path) as src:\n",
    "            mask = src.read(1).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Data Augmentation\n",
    "mean_vals = (0.5, 0.5, 0.5, 0.5)\n",
    "std_vals = (0.5, 0.5, 0.5, 0.5)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(513, 513),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=mean_vals, std=std_vals, max_pixel_value=65535.0),\n",
    "], additional_targets={'mask': 'mask'})\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(513, 513),\n",
    "    A.Normalize(mean=mean_vals, std=std_vals, max_pixel_value=65535.0),\n",
    "], additional_targets={'mask': 'mask'})\n",
    "\n",
    "# Load and Split Dataset\n",
    "root_dir = '/kaggle/input/cloud-cover-detection/data'\n",
    "metadata = pd.read_csv(os.path.join('/kaggle/input/cloud-cover-detection/train_metadata.csv'))\n",
    "train_meta, temp_meta = train_test_split(metadata, test_size=0.3, random_state=42)\n",
    "val_meta, test_meta = train_test_split(temp_meta, test_size=2/3, random_state=42)\n",
    "\n",
    "train_dataset = CloudDataset(train_meta, root_dir, transform=train_transform)\n",
    "val_dataset = CloudDataset(val_meta, root_dir, transform=val_transform)\n",
    "test_dataset = CloudDataset(test_meta, root_dir, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "# Lightweight Xception Backbone with 4-channel input\n",
    "class XceptionBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XceptionBackbone, self).__init__()\n",
    "        self.model = timm.create_model('xception41', pretrained=True, features_only=True, in_chans=4)\n",
    "        self.pool1 = nn.AvgPool2d(2, stride=2)\n",
    "        self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "        self.pool3 = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        low_level = features[1]\n",
    "        out = features[3]\n",
    "        out = self.pool1(out)\n",
    "        out = self.pool2(out)\n",
    "        out = self.pool3(out)\n",
    "        return {'out': out, 'low_level': low_level}\n",
    "\n",
    "# ASPP\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.aspp1 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1, dilation=1, bias=False),\n",
    "                                   nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "        self.aspp2 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=3, dilation=3, bias=False),\n",
    "                                   nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "        self.aspp3 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=9, dilation=9, bias=False),\n",
    "                                   nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "        self.aspp4 = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12, bias=False),\n",
    "                                   nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                             nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "                                             nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "        self.conv1 = nn.Conv2d(out_channels * 5, out_channels, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "# CBAM\n",
    "class ImprovedCBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(ImprovedCBAM, self).__init__()\n",
    "        self.channel_attention = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n",
    "                                               nn.Conv2d(channels, channels // reduction, 1),\n",
    "                                               nn.ReLU(),\n",
    "                                               nn.Conv2d(channels // reduction, channels, 1),\n",
    "                                               nn.Sigmoid())\n",
    "        self.spatial_attention = nn.Sequential(nn.Conv2d(2, 1, 7, padding=3), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        ca = self.channel_attention(x)\n",
    "        x = x * ca + x\n",
    "        sa = torch.cat([torch.max(x, 1, keepdim=True)[0], torch.mean(x, 1, keepdim=True)], dim=1)\n",
    "        sa = self.spatial_attention(sa)\n",
    "        return x * sa\n",
    "\n",
    "# GAU\n",
    "class GAU(nn.Module):\n",
    "    def __init__(self, low_channels, high_channels):\n",
    "        super(GAU, self).__init__()\n",
    "        self.conv_low = nn.Conv2d(low_channels, 256, 3, padding=1, bias=False)\n",
    "        self.bn_low = nn.BatchNorm2d(256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_weight = nn.Conv2d(high_channels, 256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, low_features, high_features):\n",
    "        low = self.conv_low(low_features)\n",
    "        low = self.bn_low(low)\n",
    "        low = self.relu(low)\n",
    "        high = F.interpolate(high_features, size=low.size()[2:], mode='bilinear', align_corners=False)\n",
    "        weights = self.global_pool(high_features)\n",
    "        weights = self.conv_weight(weights)\n",
    "        weights = self.sigmoid(weights)\n",
    "        low = low * weights\n",
    "        return low + high\n",
    "\n",
    "# Final Model\n",
    "class ImprovedDeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(ImprovedDeepLabV3Plus, self).__init__()\n",
    "        self.backbone = XceptionBackbone()\n",
    "        self.aspp = ASPP(in_channels=1024, out_channels=256)\n",
    "        self.cbam = ImprovedCBAM(256)\n",
    "        self.gau = GAU(low_channels=256, high_channels=256)\n",
    "        self.final_conv = nn.Conv2d(256, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_shape = x.shape[-2:]\n",
    "        features = self.backbone(x)\n",
    "        low_level = features['low_level']\n",
    "        x = features['out']\n",
    "        x = self.aspp(x)\n",
    "        x = self.cbam(x)\n",
    "        x = self.gau(low_level, x)\n",
    "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
    "        return torch.sigmoid(self.final_conv(x))\n",
    "\n",
    "# Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        smooth = 1.\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        intersection = (pred * target).sum()\n",
    "        return 1 - ((2. * intersection + smooth) /\n",
    "                    (pred.sum() + target.sum() + smooth))\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    TP = (pred * target).sum()\n",
    "    TN = ((1 - pred) * (1 - target)).sum()\n",
    "    FP = (pred * (1 - target)).sum()\n",
    "    FN = ((1 - pred) * target).sum()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-6)\n",
    "    precision = TP / (TP + FP + 1e-6)\n",
    "    recall = TP / (TP + FN + 1e-6)\n",
    "    iou = TP / (TP + FP + FN + 1e-6)\n",
    "    return accuracy.item(), precision.item(), recall.item(), iou.item()\n",
    "\n",
    "# Training with Mixed Precision and Validation Fix\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    with open('training_results.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['epoch', 'train_loss', 'val_loss', 'accuracy', 'precision', 'recall', 'iou', 'learning_rate']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            total_accuracy = 0.0\n",
    "            total_precision = 0.0\n",
    "            total_recall = 0.0\n",
    "            total_iou = 0.0\n",
    "\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "\n",
    "            for images, masks in progress_bar:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                accuracy, precision, recall, iou = compute_metrics(outputs, masks)\n",
    "                total_accuracy += accuracy\n",
    "                total_precision += precision\n",
    "                total_recall += recall\n",
    "                total_iou += iou\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'accuracy': f'{accuracy:.4f}',\n",
    "                    'precision': f'{precision:.4f}',\n",
    "                    'recall': f'{recall:.4f}',\n",
    "                    'IoU': f'{iou:.4f}'\n",
    "                })\n",
    "\n",
    "            # Validation loss\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, masks in val_loader:\n",
    "                    images, masks = images.to(device), masks.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            avg_accuracy = total_accuracy / len(train_loader)\n",
    "            avg_precision = total_precision / len(train_loader)\n",
    "            avg_recall = total_recall / len(train_loader)\n",
    "            avg_iou = total_iou / len(train_loader)\n",
    "\n",
    "            writer.writerow({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'accuracy': avg_accuracy,\n",
    "                'precision': avg_precision,\n",
    "                'recall': avg_recall,\n",
    "                'iou': avg_iou,\n",
    "                'learning_rate': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "            csvfile.flush()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), '/kaggle/working/final1_model.pth')\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    model = ImprovedDeepLabV3Plus(num_classes=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    # Use BCEWithLogitsLoss for stability with mixed precision\n",
    "    criterion = lambda pred, target: 0.5 * nn.BCEWithLogitsLoss()(pred, target) + 0.5 * DiceLoss()(pred, target)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=15, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T08:34:40.815198Z",
     "iopub.status.busy": "2025-04-20T08:34:40.814924Z",
     "iopub.status.idle": "2025-04-20T08:36:25.087816Z",
     "shell.execute_reply": "2025-04-20T08:36:25.086765Z",
     "shell.execute_reply.started": "2025-04-20T08:34:40.815179Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reuse compute_metrics and model definition\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model\n",
    "model = ImprovedDeepLabV3Plus(num_classes=1)\n",
    "model.load_state_dict(torch.load('/kaggle/working/final1_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Metrics accumulators\n",
    "total_accuracy = 0.0\n",
    "total_precision = 0.0\n",
    "total_recall = 0.0\n",
    "total_iou = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        accuracy, precision, recall, iou = compute_metrics(outputs, masks)\n",
    "        total_accuracy += accuracy\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_iou += iou\n",
    "        num_batches += 1\n",
    "\n",
    "# Average metrics over all test batches\n",
    "avg_accuracy = total_accuracy / num_batches\n",
    "avg_precision = total_precision / num_batches\n",
    "avg_recall = total_recall / num_batches\n",
    "avg_iou = total_iou / num_batches\n",
    "\n",
    "print(\"\\n✅ Final Evaluation on Test Set:\")\n",
    "print(f\"Accuracy:  {avg_accuracy:.4f}\")\n",
    "print(f\"Precision: {avg_precision:.4f}\")\n",
    "print(f\"Recall:    {avg_recall:.4f}\")\n",
    "print(f\"IoU:       {avg_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T09:07:00.304155Z",
     "iopub.status.busy": "2025-04-20T09:07:00.303814Z",
     "iopub.status.idle": "2025-04-20T09:07:00.332889Z",
     "shell.execute_reply": "2025-04-20T09:07:00.332001Z",
     "shell.execute_reply.started": "2025-04-20T09:07:00.304129Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": [avg_accuracy],\n",
    "    \"Precision\": [avg_precision],\n",
    "    \"Recall\": [avg_recall],\n",
    "    \"IoU\": [avg_iou]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df.to_csv(\"/kaggle/working/test_metrics.csv\", index=False)\n",
    "\n",
    "print(\"✅ Test set metrics saved to: /kaggle/working/test_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T08:58:52.857106Z",
     "iopub.status.busy": "2025-04-20T08:58:52.856401Z",
     "iopub.status.idle": "2025-04-20T08:58:56.735418Z",
     "shell.execute_reply": "2025-04-20T08:58:56.73452Z",
     "shell.execute_reply.started": "2025-04-20T08:58:52.857081Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Function to visualize predictions\n",
    "# ---------------------------\n",
    "def visualize_predictions(model, test_loader, model_path, num_images=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    shown = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = (outputs > 0.5).float()\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                img = images[i].cpu().numpy()\n",
    "                mask = masks[i][0].cpu().numpy()\n",
    "                pred = preds[i][0].cpu().numpy()\n",
    "\n",
    "                # Normalize image to 0-1\n",
    "                img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "\n",
    "                # Use B04 (Red), B03 (Green), B02 (Blue)\n",
    "                rgb = img[[2, 1, 0], :, :].transpose(1, 2, 0)\n",
    "\n",
    "                # Plot\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                axs[0].imshow(rgb)\n",
    "                axs[0].set_title(\"RGB Image (B04, B03, B02)\")\n",
    "\n",
    "                axs[1].imshow(mask, cmap='gray')\n",
    "                axs[1].set_title(\"Ground Truth\")\n",
    "\n",
    "                axs[2].imshow(pred, cmap='gray')\n",
    "                axs[2].set_title(\"Predicted Mask\")\n",
    "\n",
    "                for ax in axs:\n",
    "                    ax.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                shown += 1\n",
    "                if shown >= num_images:\n",
    "                    return\n",
    "\n",
    "# ---------------------------\n",
    "# Usage\n",
    "# ---------------------------\n",
    "\n",
    "# Make sure your model class is defined and test_loader is ready\n",
    "model = ImprovedDeepLabV3Plus(num_classes=1)\n",
    "model_path = '/kaggle/working/final1_model.pth'  # Update if saved elsewhere\n",
    "\n",
    "visualize_predictions(model, test_loader, model_path, num_images=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T09:49:31.126354Z",
     "iopub.status.busy": "2025-04-20T09:49:31.126093Z",
     "iopub.status.idle": "2025-04-20T09:49:32.323565Z",
     "shell.execute_reply": "2025-04-20T09:49:32.322831Z",
     "shell.execute_reply.started": "2025-04-20T09:49:31.126336Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training results CSV file\n",
    "training_results = pd.read_csv('/kaggle/working/training_results.csv')\n",
    "\n",
    "# Load the model (replace with your actual model class)\n",
    "model_path = '/kaggle/working/final1_model.pth'  # Path to the saved best model\n",
    "model = ImprovedDeepLabV3Plus(num_classes=1)  # Use the same model class you used during training\n",
    "# Load the model with weights_only=True to avoid potential security risks\n",
    "model.load_state_dict(torch.load('/kaggle/working/final1_model.pth', weights_only=True))\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Plot Loss (Training vs Validation) and save\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_results['epoch'], training_results['train_loss'], label='Train Loss', color='blue')\n",
    "plt.plot(training_results['epoch'], training_results['val_loss'], label='Validation Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('/kaggle/working/training_vs_validation_loss.png')  # Save to Kaggle working directory\n",
    "plt.show()  # Display the plot\n",
    "plt.close()\n",
    "\n",
    "# Plot Accuracy, Precision, Recall, IoU over epochs and save\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_results['epoch'], training_results['accuracy'], label='Accuracy', color='green')\n",
    "plt.plot(training_results['epoch'], training_results['precision'], label='Precision', color='purple')\n",
    "plt.plot(training_results['epoch'], training_results['recall'], label='Recall', color='orange')\n",
    "plt.plot(training_results['epoch'], training_results['iou'], label='IoU', color='brown')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Training Metrics (Accuracy, Precision, Recall, IoU)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('/kaggle/working/training_metrics.png')  # Save to Kaggle working directory\n",
    "plt.show()  # Display the plot\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1769893,
     "sourceId": 2892251,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
